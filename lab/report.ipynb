{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Published\" as CLIENT\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"client_KYOM4YFDIE4TMEO3UAOTEPRC-55C7JZ62MGB2UXA6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anvil.tables import app_tables\n",
    "\n",
    "answers = app_tables.answers.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created_at': datetime.datetime(2024, 8, 15, 16, 44, 24, 571000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Entropy is a measure of the average uncertainty in a set of outcomes, while cross-entropy measures the difference between two probability distributions.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': False,\n",
       "  'session': '10071df2-2c28-4c47-896d-61ac89d3883a',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 16, 44, 29, 268000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Cross-entropy loss measures the difference between predicted probabilities and actual outcomes, while entropy quantifies the uncertainty in a probability distribution.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': True,\n",
       "  'session': '10071df2-2c28-4c47-896d-61ac89d3883a',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 16, 44, 31, 959000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Entropy is a measure of uncertainty in a probability distribution, while cross-entropy measures the difference between two probability distributions.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': True,\n",
       "  'session': '10071df2-2c28-4c47-896d-61ac89d3883a',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 16, 44, 35, 482000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Cross-entropy loss is the same as perplexity, used to evaluate language models.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': True,\n",
       "  'session': '10071df2-2c28-4c47-896d-61ac89d3883a',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 16, 45, 10, 548000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Entropy is a measure of the average uncertainty in a set of outcomes, while cross-entropy measures the difference between two probability distributions.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': False,\n",
       "  'session': 'a25f3257-1470-41d8-07a6-3a23cf8c1512',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 16, 45, 13, 482000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Cross-entropy loss measures the difference between predicted probabilities and actual outcomes, while entropy quantifies the uncertainty in a probability distribution.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': True,\n",
       "  'session': 'a25f3257-1470-41d8-07a6-3a23cf8c1512',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 17, 0, 4, 336000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'A linear transformation preserves linear combinations of vectors, but not their magnitudes.',\n",
       "  'question_title': 'Linear Algebra - Basics',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'medium',\n",
       "  'got_it_right': True,\n",
       "  'session': 'a04dda00-ef64-8199-bf50-dd68689cccba',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 15, 17, 0, 28, 857000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Cross-Entropy is always less than or equal to Entropy for a given probability distribution.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'medium',\n",
       "  'got_it_right': False,\n",
       "  'session': 'a04dda00-ef64-8199-bf50-dd68689cccba',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 21, 16, 23, 20, 760000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': \"Perplexity is used as a loss function in machine learning models to optimize the model's performance.\",\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'medium',\n",
       "  'got_it_right': True,\n",
       "  'session': '94653bb8-c306-1ec1-0506-82af1a6fb586',\n",
       "  'user': 'teste@teste.com'},\n",
       " {'created_at': datetime.datetime(2024, 8, 21, 16, 42, 17, 874000, tzinfo=<anvil.tz.tzoffset (-3.0 hour offset)>),\n",
       "  'question_question': 'Minimizing cross-entropy loss during training improves the performance of a language model.',\n",
       "  'question_title': 'Probability - LLMs',\n",
       "  'question_type': 'true_or_false',\n",
       "  'question_level': 'easy',\n",
       "  'got_it_right': True,\n",
       "  'session': '5e189bba-fdf0-b5cb-fb32-3658f4918466',\n",
       "  'user': 'teste@teste.com'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_list = [ {\n",
    "    \"created_at\": r[\"created_at\"],\n",
    "    \"question_question\": r[\"question\"][\"question\"],\n",
    "    \"question_title\": r[\"question\"][\"title\"],\n",
    "    \"question_type\": r[\"question\"][\"type\"],\n",
    "    \"question_level\": r[\"question\"][\"level\"],\n",
    "    \"got_it_right\": r[\"got_it_right\"],\n",
    "    \"session\":r[\"session\"],\n",
    "    \"user\": r[\"user\"][\"email\"]\n",
    "    } for r in answers\n",
    "    ]\n",
    "\n",
    "answers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>question_question</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_type</th>\n",
       "      <th>question_level</th>\n",
       "      <th>got_it_right</th>\n",
       "      <th>session</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-15 16:44:24.571000-03:00</td>\n",
       "      <td>Entropy is a measure of the average uncertaint...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>False</td>\n",
       "      <td>10071df2-2c28-4c47-896d-61ac89d3883a</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-15 16:44:29.268000-03:00</td>\n",
       "      <td>Cross-entropy loss measures the difference bet...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>True</td>\n",
       "      <td>10071df2-2c28-4c47-896d-61ac89d3883a</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-15 16:44:31.959000-03:00</td>\n",
       "      <td>Entropy is a measure of uncertainty in a proba...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>True</td>\n",
       "      <td>10071df2-2c28-4c47-896d-61ac89d3883a</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-15 16:44:35.482000-03:00</td>\n",
       "      <td>Cross-entropy loss is the same as perplexity, ...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>True</td>\n",
       "      <td>10071df2-2c28-4c47-896d-61ac89d3883a</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-15 16:45:10.548000-03:00</td>\n",
       "      <td>Entropy is a measure of the average uncertaint...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>False</td>\n",
       "      <td>a25f3257-1470-41d8-07a6-3a23cf8c1512</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-08-15 16:45:13.482000-03:00</td>\n",
       "      <td>Cross-entropy loss measures the difference bet...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>True</td>\n",
       "      <td>a25f3257-1470-41d8-07a6-3a23cf8c1512</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-08-15 17:00:04.336000-03:00</td>\n",
       "      <td>A linear transformation preserves linear combi...</td>\n",
       "      <td>Linear Algebra - Basics</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>medium</td>\n",
       "      <td>True</td>\n",
       "      <td>a04dda00-ef64-8199-bf50-dd68689cccba</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-08-15 17:00:28.857000-03:00</td>\n",
       "      <td>Cross-Entropy is always less than or equal to ...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>a04dda00-ef64-8199-bf50-dd68689cccba</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-08-21 16:23:20.760000-03:00</td>\n",
       "      <td>Perplexity is used as a loss function in machi...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>medium</td>\n",
       "      <td>True</td>\n",
       "      <td>94653bb8-c306-1ec1-0506-82af1a6fb586</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-08-21 16:42:17.874000-03:00</td>\n",
       "      <td>Minimizing cross-entropy loss during training ...</td>\n",
       "      <td>Probability - LLMs</td>\n",
       "      <td>true_or_false</td>\n",
       "      <td>easy</td>\n",
       "      <td>True</td>\n",
       "      <td>5e189bba-fdf0-b5cb-fb32-3658f4918466</td>\n",
       "      <td>teste@teste.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         created_at  \\\n",
       "0  2024-08-15 16:44:24.571000-03:00   \n",
       "1  2024-08-15 16:44:29.268000-03:00   \n",
       "2  2024-08-15 16:44:31.959000-03:00   \n",
       "3  2024-08-15 16:44:35.482000-03:00   \n",
       "4  2024-08-15 16:45:10.548000-03:00   \n",
       "5  2024-08-15 16:45:13.482000-03:00   \n",
       "6  2024-08-15 17:00:04.336000-03:00   \n",
       "7  2024-08-15 17:00:28.857000-03:00   \n",
       "8  2024-08-21 16:23:20.760000-03:00   \n",
       "9  2024-08-21 16:42:17.874000-03:00   \n",
       "\n",
       "                                   question_question           question_title  \\\n",
       "0  Entropy is a measure of the average uncertaint...       Probability - LLMs   \n",
       "1  Cross-entropy loss measures the difference bet...       Probability - LLMs   \n",
       "2  Entropy is a measure of uncertainty in a proba...       Probability - LLMs   \n",
       "3  Cross-entropy loss is the same as perplexity, ...       Probability - LLMs   \n",
       "4  Entropy is a measure of the average uncertaint...       Probability - LLMs   \n",
       "5  Cross-entropy loss measures the difference bet...       Probability - LLMs   \n",
       "6  A linear transformation preserves linear combi...  Linear Algebra - Basics   \n",
       "7  Cross-Entropy is always less than or equal to ...       Probability - LLMs   \n",
       "8  Perplexity is used as a loss function in machi...       Probability - LLMs   \n",
       "9  Minimizing cross-entropy loss during training ...       Probability - LLMs   \n",
       "\n",
       "   question_type question_level  got_it_right  \\\n",
       "0  true_or_false           easy         False   \n",
       "1  true_or_false           easy          True   \n",
       "2  true_or_false           easy          True   \n",
       "3  true_or_false           easy          True   \n",
       "4  true_or_false           easy         False   \n",
       "5  true_or_false           easy          True   \n",
       "6  true_or_false         medium          True   \n",
       "7  true_or_false         medium         False   \n",
       "8  true_or_false         medium          True   \n",
       "9  true_or_false           easy          True   \n",
       "\n",
       "                                session             user  \n",
       "0  10071df2-2c28-4c47-896d-61ac89d3883a  teste@teste.com  \n",
       "1  10071df2-2c28-4c47-896d-61ac89d3883a  teste@teste.com  \n",
       "2  10071df2-2c28-4c47-896d-61ac89d3883a  teste@teste.com  \n",
       "3  10071df2-2c28-4c47-896d-61ac89d3883a  teste@teste.com  \n",
       "4  a25f3257-1470-41d8-07a6-3a23cf8c1512  teste@teste.com  \n",
       "5  a25f3257-1470-41d8-07a6-3a23cf8c1512  teste@teste.com  \n",
       "6  a04dda00-ef64-8199-bf50-dd68689cccba  teste@teste.com  \n",
       "7  a04dda00-ef64-8199-bf50-dd68689cccba  teste@teste.com  \n",
       "8  94653bb8-c306-1ec1-0506-82af1a6fb586  teste@teste.com  \n",
       "9  5e189bba-fdf0-b5cb-fb32-3658f4918466  teste@teste.com  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(answers_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>got_it_right</th>\n",
       "      <th>Wrong</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Probability - LLMs</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Algebra - Basics</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "got_it_right             Wrong  Right\n",
       "question_title                       \n",
       "Probability - LLMs           3      6\n",
       "Linear Algebra - Basics      0      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df.groupby([\"question_title\"])[\"got_it_right\"].value_counts().unstack(fill_value=0)\n",
    "df_ = df_.rename(columns={False: 'Wrong', True: 'Right'})\n",
    "df_ = df_.sort_values(by=\"Wrong\", ascending=False)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>got_it_right</th>\n",
       "      <th>Wrong</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_title</th>\n",
       "      <th>question_question</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Probability - LLMs</th>\n",
       "      <th>Entropy is a measure of the average uncertainty in a set of outcomes, while cross-entropy measures the difference between two probability distributions.</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Entropy is always less than or equal to Entropy for a given probability distribution.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-entropy loss is the same as perplexity, used to evaluate language models.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Algebra - Basics</th>\n",
       "      <th>A linear transformation preserves linear combinations of vectors, but not their magnitudes.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Probability - LLMs</th>\n",
       "      <th>Cross-entropy loss measures the difference between predicted probabilities and actual outcomes, while entropy quantifies the uncertainty in a probability distribution.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entropy is a measure of uncertainty in a probability distribution, while cross-entropy measures the difference between two probability distributions.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimizing cross-entropy loss during training improves the performance of a language model.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity is used as a loss function in machine learning models to optimize the model's performance.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "got_it_right                                                                Wrong  \\\n",
       "question_title          question_question                                           \n",
       "Probability - LLMs      Entropy is a measure of the average uncertainty...      2   \n",
       "                        Cross-Entropy is always less than or equal to E...      1   \n",
       "                        Cross-entropy loss is the same as perplexity, u...      0   \n",
       "Linear Algebra - Basics A linear transformation preserves linear combin...      0   \n",
       "Probability - LLMs      Cross-entropy loss measures the difference betw...      0   \n",
       "                        Entropy is a measure of uncertainty in a probab...      0   \n",
       "                        Minimizing cross-entropy loss during training i...      0   \n",
       "                        Perplexity is used as a loss function in machin...      0   \n",
       "\n",
       "got_it_right                                                                Right  \n",
       "question_title          question_question                                          \n",
       "Probability - LLMs      Entropy is a measure of the average uncertainty...      0  \n",
       "                        Cross-Entropy is always less than or equal to E...      0  \n",
       "                        Cross-entropy loss is the same as perplexity, u...      1  \n",
       "Linear Algebra - Basics A linear transformation preserves linear combin...      1  \n",
       "Probability - LLMs      Cross-entropy loss measures the difference betw...      2  \n",
       "                        Entropy is a measure of uncertainty in a probab...      1  \n",
       "                        Minimizing cross-entropy loss during training i...      1  \n",
       "                        Perplexity is used as a loss function in machin...      1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_= df.groupby([\"question_title\", \"question_question\"])[\"got_it_right\"].value_counts().unstack(fill_value=0)\n",
    "df_ = df_.rename(columns={False: 'Wrong', True: 'Right'})\n",
    "df_ = df_.sort_values(by=\"Wrong\", ascending=False)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
